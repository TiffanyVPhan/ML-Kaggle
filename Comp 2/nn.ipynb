{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG Data Size:(12379, 226)\n",
      "New Data Size:(6030, 188)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning Data\n",
    "\n",
    "df = pd.read_csv('data/stock_XY_train.csv')\n",
    "print('OG Data Size:{}'.format(df.shape))\n",
    "\n",
    "df = df[df.columns[df.isnull().mean() < 0.3]] # TO-DO: Tinker around with mean threshold.\n",
    "df = df.dropna()\n",
    "print('New Data Size:{}'.format(df.shape))\n",
    "del df['operatingProfitMargin'] # Got rid of this column because it is all `1`. No reason to keep.\n",
    "\n",
    "del df['Ticker']\n",
    "del df['Sector']\n",
    "\n",
    "# dropping for now cuz this hot encoding is annoying. \n",
    "# TO-DO: Get hot encoding working for Sector + Ticker columns\n",
    "# pd.get_dummies(df, columns = ['Sector', 'Ticker'])\n",
    "# print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating to train and test data\n",
    "train_df = df.sample(frac=0.8,random_state=0)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Grabbing stats in order to normalize data\n",
    "train_stats = train_df.describe()\n",
    "train_stats.pop('Buy')\n",
    "train_stats = train_stats.transpose()\n",
    "\n",
    "# Separating labels\n",
    "train_label = train_df.pop('Buy')\n",
    "test_label = test_df.pop('Buy')\n",
    "\n",
    "# Normalizing Data\n",
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "normed_train_data = norm(train_df)\n",
    "normed_test_data = norm(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 64)                11840     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 16,065\n",
      "Trainable params: 16,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=[len(train_df.keys())]),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "                    learning_rate=0.001, \n",
    "                    beta_1=0.9, \n",
    "                    beta_2=0.999, \n",
    "                    epsilon=1e-07, \n",
    "                    amsgrad=False,\n",
    "                    name='Adam')\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse', 'accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "history = model.fit(\n",
    "  normed_train_data, train_labels,\n",
    "  epochs=EPOCHS, validation_split = 0.2, verbose=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.520347</td>\n",
       "      <td>0.552337</td>\n",
       "      <td>0.520347</td>\n",
       "      <td>0.502462</td>\n",
       "      <td>0.316236</td>\n",
       "      <td>0.501488</td>\n",
       "      <td>0.316236</td>\n",
       "      <td>0.538860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.454838</td>\n",
       "      <td>0.495181</td>\n",
       "      <td>0.454838</td>\n",
       "      <td>0.584348</td>\n",
       "      <td>0.322664</td>\n",
       "      <td>0.491624</td>\n",
       "      <td>0.322664</td>\n",
       "      <td>0.583420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.695899</td>\n",
       "      <td>0.498929</td>\n",
       "      <td>0.695899</td>\n",
       "      <td>0.600933</td>\n",
       "      <td>0.406425</td>\n",
       "      <td>0.490341</td>\n",
       "      <td>0.406425</td>\n",
       "      <td>0.604145</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.444578</td>\n",
       "      <td>0.466476</td>\n",
       "      <td>0.444578</td>\n",
       "      <td>0.644467</td>\n",
       "      <td>0.250883</td>\n",
       "      <td>0.456547</td>\n",
       "      <td>0.250883</td>\n",
       "      <td>0.619689</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.302687</td>\n",
       "      <td>0.444945</td>\n",
       "      <td>0.302687</td>\n",
       "      <td>0.670899</td>\n",
       "      <td>0.286613</td>\n",
       "      <td>0.472490</td>\n",
       "      <td>0.286613</td>\n",
       "      <td>0.622798</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.265774</td>\n",
       "      <td>0.431594</td>\n",
       "      <td>0.265774</td>\n",
       "      <td>0.683856</td>\n",
       "      <td>0.249688</td>\n",
       "      <td>0.451508</td>\n",
       "      <td>0.249688</td>\n",
       "      <td>0.644560</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.265209</td>\n",
       "      <td>0.420058</td>\n",
       "      <td>0.265209</td>\n",
       "      <td>0.698886</td>\n",
       "      <td>0.252444</td>\n",
       "      <td>0.457889</td>\n",
       "      <td>0.252444</td>\n",
       "      <td>0.646632</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.261960</td>\n",
       "      <td>0.420434</td>\n",
       "      <td>0.261960</td>\n",
       "      <td>0.703809</td>\n",
       "      <td>0.260153</td>\n",
       "      <td>0.440372</td>\n",
       "      <td>0.260153</td>\n",
       "      <td>0.638342</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.243129</td>\n",
       "      <td>0.411148</td>\n",
       "      <td>0.243129</td>\n",
       "      <td>0.706919</td>\n",
       "      <td>0.241719</td>\n",
       "      <td>0.443542</td>\n",
       "      <td>0.241719</td>\n",
       "      <td>0.640415</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.199984</td>\n",
       "      <td>0.395529</td>\n",
       "      <td>0.199984</td>\n",
       "      <td>0.718839</td>\n",
       "      <td>0.244681</td>\n",
       "      <td>0.438630</td>\n",
       "      <td>0.244681</td>\n",
       "      <td>0.663212</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.185535</td>\n",
       "      <td>0.385836</td>\n",
       "      <td>0.185535</td>\n",
       "      <td>0.734646</td>\n",
       "      <td>0.241440</td>\n",
       "      <td>0.431843</td>\n",
       "      <td>0.241440</td>\n",
       "      <td>0.647668</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.178355</td>\n",
       "      <td>0.377364</td>\n",
       "      <td>0.178355</td>\n",
       "      <td>0.734387</td>\n",
       "      <td>0.247518</td>\n",
       "      <td>0.435035</td>\n",
       "      <td>0.247518</td>\n",
       "      <td>0.654922</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.174848</td>\n",
       "      <td>0.371667</td>\n",
       "      <td>0.174848</td>\n",
       "      <td>0.739311</td>\n",
       "      <td>0.257204</td>\n",
       "      <td>0.437038</td>\n",
       "      <td>0.257204</td>\n",
       "      <td>0.645596</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.172409</td>\n",
       "      <td>0.367592</td>\n",
       "      <td>0.172409</td>\n",
       "      <td>0.749935</td>\n",
       "      <td>0.248361</td>\n",
       "      <td>0.432193</td>\n",
       "      <td>0.248361</td>\n",
       "      <td>0.653886</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.171032</td>\n",
       "      <td>0.365651</td>\n",
       "      <td>0.171032</td>\n",
       "      <td>0.752008</td>\n",
       "      <td>0.262475</td>\n",
       "      <td>0.432861</td>\n",
       "      <td>0.262475</td>\n",
       "      <td>0.658031</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.172953</td>\n",
       "      <td>0.363855</td>\n",
       "      <td>0.172953</td>\n",
       "      <td>0.751749</td>\n",
       "      <td>0.261119</td>\n",
       "      <td>0.430813</td>\n",
       "      <td>0.261119</td>\n",
       "      <td>0.653886</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.174292</td>\n",
       "      <td>0.362078</td>\n",
       "      <td>0.174292</td>\n",
       "      <td>0.752008</td>\n",
       "      <td>0.265139</td>\n",
       "      <td>0.437718</td>\n",
       "      <td>0.265139</td>\n",
       "      <td>0.641451</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.176555</td>\n",
       "      <td>0.364518</td>\n",
       "      <td>0.176555</td>\n",
       "      <td>0.753045</td>\n",
       "      <td>0.263109</td>\n",
       "      <td>0.436768</td>\n",
       "      <td>0.263109</td>\n",
       "      <td>0.641451</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.178370</td>\n",
       "      <td>0.362813</td>\n",
       "      <td>0.178370</td>\n",
       "      <td>0.763669</td>\n",
       "      <td>0.272346</td>\n",
       "      <td>0.437083</td>\n",
       "      <td>0.272346</td>\n",
       "      <td>0.640415</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.184988</td>\n",
       "      <td>0.362001</td>\n",
       "      <td>0.184988</td>\n",
       "      <td>0.769889</td>\n",
       "      <td>0.283589</td>\n",
       "      <td>0.446760</td>\n",
       "      <td>0.283589</td>\n",
       "      <td>0.632124</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.187240</td>\n",
       "      <td>0.355876</td>\n",
       "      <td>0.187240</td>\n",
       "      <td>0.773776</td>\n",
       "      <td>0.271826</td>\n",
       "      <td>0.433598</td>\n",
       "      <td>0.271826</td>\n",
       "      <td>0.649741</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.182398</td>\n",
       "      <td>0.351863</td>\n",
       "      <td>0.182397</td>\n",
       "      <td>0.767297</td>\n",
       "      <td>0.282528</td>\n",
       "      <td>0.438621</td>\n",
       "      <td>0.282528</td>\n",
       "      <td>0.633161</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.213529</td>\n",
       "      <td>0.357452</td>\n",
       "      <td>0.213529</td>\n",
       "      <td>0.770148</td>\n",
       "      <td>0.284735</td>\n",
       "      <td>0.442502</td>\n",
       "      <td>0.284735</td>\n",
       "      <td>0.635233</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.221026</td>\n",
       "      <td>0.347171</td>\n",
       "      <td>0.221026</td>\n",
       "      <td>0.780772</td>\n",
       "      <td>0.284410</td>\n",
       "      <td>0.443545</td>\n",
       "      <td>0.284410</td>\n",
       "      <td>0.623834</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.176284</td>\n",
       "      <td>0.343768</td>\n",
       "      <td>0.176284</td>\n",
       "      <td>0.782068</td>\n",
       "      <td>0.282924</td>\n",
       "      <td>0.441076</td>\n",
       "      <td>0.282924</td>\n",
       "      <td>0.642487</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.171933</td>\n",
       "      <td>0.336655</td>\n",
       "      <td>0.171933</td>\n",
       "      <td>0.788805</td>\n",
       "      <td>0.278783</td>\n",
       "      <td>0.440627</td>\n",
       "      <td>0.278783</td>\n",
       "      <td>0.647668</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.155029</td>\n",
       "      <td>0.332139</td>\n",
       "      <td>0.155029</td>\n",
       "      <td>0.789842</td>\n",
       "      <td>0.275450</td>\n",
       "      <td>0.437253</td>\n",
       "      <td>0.275450</td>\n",
       "      <td>0.636269</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.151617</td>\n",
       "      <td>0.326396</td>\n",
       "      <td>0.151617</td>\n",
       "      <td>0.800207</td>\n",
       "      <td>0.290119</td>\n",
       "      <td>0.442603</td>\n",
       "      <td>0.290119</td>\n",
       "      <td>0.637306</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.146623</td>\n",
       "      <td>0.323706</td>\n",
       "      <td>0.146623</td>\n",
       "      <td>0.797875</td>\n",
       "      <td>0.293054</td>\n",
       "      <td>0.441134</td>\n",
       "      <td>0.293054</td>\n",
       "      <td>0.631088</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.145412</td>\n",
       "      <td>0.321068</td>\n",
       "      <td>0.145412</td>\n",
       "      <td>0.800726</td>\n",
       "      <td>0.280624</td>\n",
       "      <td>0.429089</td>\n",
       "      <td>0.280624</td>\n",
       "      <td>0.640415</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.146809</td>\n",
       "      <td>0.319062</td>\n",
       "      <td>0.146809</td>\n",
       "      <td>0.798134</td>\n",
       "      <td>0.304120</td>\n",
       "      <td>0.442576</td>\n",
       "      <td>0.304120</td>\n",
       "      <td>0.635233</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.142902</td>\n",
       "      <td>0.317803</td>\n",
       "      <td>0.142902</td>\n",
       "      <td>0.808500</td>\n",
       "      <td>0.305393</td>\n",
       "      <td>0.452055</td>\n",
       "      <td>0.305393</td>\n",
       "      <td>0.606218</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.147912</td>\n",
       "      <td>0.319935</td>\n",
       "      <td>0.147912</td>\n",
       "      <td>0.805131</td>\n",
       "      <td>0.311074</td>\n",
       "      <td>0.452209</td>\n",
       "      <td>0.311074</td>\n",
       "      <td>0.619689</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.145579</td>\n",
       "      <td>0.319990</td>\n",
       "      <td>0.145579</td>\n",
       "      <td>0.811091</td>\n",
       "      <td>0.304765</td>\n",
       "      <td>0.440190</td>\n",
       "      <td>0.304765</td>\n",
       "      <td>0.627979</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.145954</td>\n",
       "      <td>0.317363</td>\n",
       "      <td>0.145954</td>\n",
       "      <td>0.812127</td>\n",
       "      <td>0.304182</td>\n",
       "      <td>0.447471</td>\n",
       "      <td>0.304182</td>\n",
       "      <td>0.618653</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.311979</td>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.810832</td>\n",
       "      <td>0.304576</td>\n",
       "      <td>0.445671</td>\n",
       "      <td>0.304576</td>\n",
       "      <td>0.625907</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.138166</td>\n",
       "      <td>0.307870</td>\n",
       "      <td>0.138166</td>\n",
       "      <td>0.818606</td>\n",
       "      <td>0.294179</td>\n",
       "      <td>0.443598</td>\n",
       "      <td>0.294179</td>\n",
       "      <td>0.616580</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.133141</td>\n",
       "      <td>0.301944</td>\n",
       "      <td>0.133141</td>\n",
       "      <td>0.825343</td>\n",
       "      <td>0.285918</td>\n",
       "      <td>0.433358</td>\n",
       "      <td>0.285918</td>\n",
       "      <td>0.631088</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.134610</td>\n",
       "      <td>0.300304</td>\n",
       "      <td>0.134610</td>\n",
       "      <td>0.828453</td>\n",
       "      <td>0.346460</td>\n",
       "      <td>0.457151</td>\n",
       "      <td>0.346460</td>\n",
       "      <td>0.615544</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.138952</td>\n",
       "      <td>0.302036</td>\n",
       "      <td>0.138952</td>\n",
       "      <td>0.827157</td>\n",
       "      <td>0.290811</td>\n",
       "      <td>0.440973</td>\n",
       "      <td>0.290811</td>\n",
       "      <td>0.627979</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.139080</td>\n",
       "      <td>0.304806</td>\n",
       "      <td>0.139080</td>\n",
       "      <td>0.828453</td>\n",
       "      <td>0.298607</td>\n",
       "      <td>0.437821</td>\n",
       "      <td>0.298607</td>\n",
       "      <td>0.622798</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.131902</td>\n",
       "      <td>0.296817</td>\n",
       "      <td>0.131902</td>\n",
       "      <td>0.831563</td>\n",
       "      <td>0.312819</td>\n",
       "      <td>0.450928</td>\n",
       "      <td>0.312819</td>\n",
       "      <td>0.608290</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.128360</td>\n",
       "      <td>0.292820</td>\n",
       "      <td>0.128360</td>\n",
       "      <td>0.838818</td>\n",
       "      <td>0.308396</td>\n",
       "      <td>0.449673</td>\n",
       "      <td>0.308396</td>\n",
       "      <td>0.609326</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.123467</td>\n",
       "      <td>0.287456</td>\n",
       "      <td>0.123467</td>\n",
       "      <td>0.845556</td>\n",
       "      <td>0.321950</td>\n",
       "      <td>0.456315</td>\n",
       "      <td>0.321950</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.122169</td>\n",
       "      <td>0.286573</td>\n",
       "      <td>0.122169</td>\n",
       "      <td>0.846852</td>\n",
       "      <td>0.318115</td>\n",
       "      <td>0.449507</td>\n",
       "      <td>0.318115</td>\n",
       "      <td>0.616580</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.126213</td>\n",
       "      <td>0.285741</td>\n",
       "      <td>0.126213</td>\n",
       "      <td>0.848925</td>\n",
       "      <td>0.312965</td>\n",
       "      <td>0.443511</td>\n",
       "      <td>0.312965</td>\n",
       "      <td>0.617617</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.130984</td>\n",
       "      <td>0.290656</td>\n",
       "      <td>0.130984</td>\n",
       "      <td>0.844519</td>\n",
       "      <td>0.296398</td>\n",
       "      <td>0.455058</td>\n",
       "      <td>0.296398</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.137016</td>\n",
       "      <td>0.294774</td>\n",
       "      <td>0.137016</td>\n",
       "      <td>0.841410</td>\n",
       "      <td>0.313312</td>\n",
       "      <td>0.452519</td>\n",
       "      <td>0.313312</td>\n",
       "      <td>0.602073</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.134573</td>\n",
       "      <td>0.292788</td>\n",
       "      <td>0.134572</td>\n",
       "      <td>0.841151</td>\n",
       "      <td>0.301042</td>\n",
       "      <td>0.445547</td>\n",
       "      <td>0.301042</td>\n",
       "      <td>0.610363</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.165503</td>\n",
       "      <td>0.287472</td>\n",
       "      <td>0.165503</td>\n",
       "      <td>0.847111</td>\n",
       "      <td>0.327389</td>\n",
       "      <td>0.457005</td>\n",
       "      <td>0.327389</td>\n",
       "      <td>0.591710</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.126144</td>\n",
       "      <td>0.284184</td>\n",
       "      <td>0.126144</td>\n",
       "      <td>0.851257</td>\n",
       "      <td>0.312080</td>\n",
       "      <td>0.455250</td>\n",
       "      <td>0.312080</td>\n",
       "      <td>0.593782</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.155782</td>\n",
       "      <td>0.283805</td>\n",
       "      <td>0.155782</td>\n",
       "      <td>0.855662</td>\n",
       "      <td>0.315460</td>\n",
       "      <td>0.461231</td>\n",
       "      <td>0.315460</td>\n",
       "      <td>0.587565</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.128029</td>\n",
       "      <td>0.277409</td>\n",
       "      <td>0.128029</td>\n",
       "      <td>0.866805</td>\n",
       "      <td>0.315137</td>\n",
       "      <td>0.463065</td>\n",
       "      <td>0.315137</td>\n",
       "      <td>0.609326</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.117441</td>\n",
       "      <td>0.275244</td>\n",
       "      <td>0.117441</td>\n",
       "      <td>0.864991</td>\n",
       "      <td>0.320473</td>\n",
       "      <td>0.461049</td>\n",
       "      <td>0.320473</td>\n",
       "      <td>0.607254</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.114263</td>\n",
       "      <td>0.271744</td>\n",
       "      <td>0.114263</td>\n",
       "      <td>0.867064</td>\n",
       "      <td>0.324848</td>\n",
       "      <td>0.461447</td>\n",
       "      <td>0.324848</td>\n",
       "      <td>0.593782</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.113146</td>\n",
       "      <td>0.267397</td>\n",
       "      <td>0.113146</td>\n",
       "      <td>0.868619</td>\n",
       "      <td>0.326166</td>\n",
       "      <td>0.454829</td>\n",
       "      <td>0.326166</td>\n",
       "      <td>0.606218</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.117217</td>\n",
       "      <td>0.269175</td>\n",
       "      <td>0.117217</td>\n",
       "      <td>0.870433</td>\n",
       "      <td>0.349644</td>\n",
       "      <td>0.467936</td>\n",
       "      <td>0.349644</td>\n",
       "      <td>0.582383</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.147392</td>\n",
       "      <td>0.277821</td>\n",
       "      <td>0.147392</td>\n",
       "      <td>0.865768</td>\n",
       "      <td>0.407535</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.407535</td>\n",
       "      <td>0.586528</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.115787</td>\n",
       "      <td>0.264909</td>\n",
       "      <td>0.115787</td>\n",
       "      <td>0.878207</td>\n",
       "      <td>0.322683</td>\n",
       "      <td>0.456025</td>\n",
       "      <td>0.322683</td>\n",
       "      <td>0.589637</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.113452</td>\n",
       "      <td>0.264480</td>\n",
       "      <td>0.113452</td>\n",
       "      <td>0.873283</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>0.457973</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>0.595855</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.108656</td>\n",
       "      <td>0.262339</td>\n",
       "      <td>0.108656</td>\n",
       "      <td>0.871210</td>\n",
       "      <td>0.326107</td>\n",
       "      <td>0.465386</td>\n",
       "      <td>0.326107</td>\n",
       "      <td>0.589637</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.101510</td>\n",
       "      <td>0.253828</td>\n",
       "      <td>0.101510</td>\n",
       "      <td>0.886240</td>\n",
       "      <td>0.335299</td>\n",
       "      <td>0.466900</td>\n",
       "      <td>0.335299</td>\n",
       "      <td>0.574093</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.101363</td>\n",
       "      <td>0.252191</td>\n",
       "      <td>0.101363</td>\n",
       "      <td>0.884426</td>\n",
       "      <td>0.346949</td>\n",
       "      <td>0.463975</td>\n",
       "      <td>0.346949</td>\n",
       "      <td>0.585492</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.102374</td>\n",
       "      <td>0.250274</td>\n",
       "      <td>0.102374</td>\n",
       "      <td>0.883908</td>\n",
       "      <td>0.342197</td>\n",
       "      <td>0.465983</td>\n",
       "      <td>0.342197</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.100814</td>\n",
       "      <td>0.251374</td>\n",
       "      <td>0.100814</td>\n",
       "      <td>0.890127</td>\n",
       "      <td>0.330664</td>\n",
       "      <td>0.456036</td>\n",
       "      <td>0.330664</td>\n",
       "      <td>0.588601</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.097096</td>\n",
       "      <td>0.246530</td>\n",
       "      <td>0.097096</td>\n",
       "      <td>0.888313</td>\n",
       "      <td>0.335145</td>\n",
       "      <td>0.464958</td>\n",
       "      <td>0.335145</td>\n",
       "      <td>0.588601</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.094367</td>\n",
       "      <td>0.241723</td>\n",
       "      <td>0.094367</td>\n",
       "      <td>0.896087</td>\n",
       "      <td>0.348889</td>\n",
       "      <td>0.467693</td>\n",
       "      <td>0.348889</td>\n",
       "      <td>0.591710</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.096191</td>\n",
       "      <td>0.245583</td>\n",
       "      <td>0.096191</td>\n",
       "      <td>0.895310</td>\n",
       "      <td>0.353573</td>\n",
       "      <td>0.471152</td>\n",
       "      <td>0.353573</td>\n",
       "      <td>0.585492</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.100730</td>\n",
       "      <td>0.246261</td>\n",
       "      <td>0.100730</td>\n",
       "      <td>0.893237</td>\n",
       "      <td>0.347921</td>\n",
       "      <td>0.469966</td>\n",
       "      <td>0.347921</td>\n",
       "      <td>0.597927</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.108966</td>\n",
       "      <td>0.252820</td>\n",
       "      <td>0.108966</td>\n",
       "      <td>0.886499</td>\n",
       "      <td>0.370134</td>\n",
       "      <td>0.475845</td>\n",
       "      <td>0.370134</td>\n",
       "      <td>0.598964</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.111371</td>\n",
       "      <td>0.255682</td>\n",
       "      <td>0.111371</td>\n",
       "      <td>0.885203</td>\n",
       "      <td>0.371120</td>\n",
       "      <td>0.482183</td>\n",
       "      <td>0.371120</td>\n",
       "      <td>0.577202</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.102001</td>\n",
       "      <td>0.246493</td>\n",
       "      <td>0.102001</td>\n",
       "      <td>0.889350</td>\n",
       "      <td>0.382253</td>\n",
       "      <td>0.480616</td>\n",
       "      <td>0.382253</td>\n",
       "      <td>0.589637</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.095462</td>\n",
       "      <td>0.243538</td>\n",
       "      <td>0.095462</td>\n",
       "      <td>0.894014</td>\n",
       "      <td>0.356529</td>\n",
       "      <td>0.476565</td>\n",
       "      <td>0.356529</td>\n",
       "      <td>0.591710</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.090229</td>\n",
       "      <td>0.235639</td>\n",
       "      <td>0.090229</td>\n",
       "      <td>0.907230</td>\n",
       "      <td>0.381325</td>\n",
       "      <td>0.480067</td>\n",
       "      <td>0.381325</td>\n",
       "      <td>0.579275</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.089881</td>\n",
       "      <td>0.232765</td>\n",
       "      <td>0.089881</td>\n",
       "      <td>0.905934</td>\n",
       "      <td>0.360463</td>\n",
       "      <td>0.478798</td>\n",
       "      <td>0.360463</td>\n",
       "      <td>0.582383</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.088535</td>\n",
       "      <td>0.231630</td>\n",
       "      <td>0.088535</td>\n",
       "      <td>0.907230</td>\n",
       "      <td>0.373845</td>\n",
       "      <td>0.484971</td>\n",
       "      <td>0.373845</td>\n",
       "      <td>0.564767</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.086556</td>\n",
       "      <td>0.228263</td>\n",
       "      <td>0.086556</td>\n",
       "      <td>0.909562</td>\n",
       "      <td>0.361072</td>\n",
       "      <td>0.476318</td>\n",
       "      <td>0.361072</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.085380</td>\n",
       "      <td>0.228040</td>\n",
       "      <td>0.085380</td>\n",
       "      <td>0.909562</td>\n",
       "      <td>0.372852</td>\n",
       "      <td>0.476610</td>\n",
       "      <td>0.372852</td>\n",
       "      <td>0.581347</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.082005</td>\n",
       "      <td>0.222452</td>\n",
       "      <td>0.082005</td>\n",
       "      <td>0.913708</td>\n",
       "      <td>0.377784</td>\n",
       "      <td>0.479639</td>\n",
       "      <td>0.377784</td>\n",
       "      <td>0.594819</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.085173</td>\n",
       "      <td>0.224505</td>\n",
       "      <td>0.085173</td>\n",
       "      <td>0.914486</td>\n",
       "      <td>0.368166</td>\n",
       "      <td>0.479887</td>\n",
       "      <td>0.368166</td>\n",
       "      <td>0.579275</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.086452</td>\n",
       "      <td>0.223505</td>\n",
       "      <td>0.086452</td>\n",
       "      <td>0.908007</td>\n",
       "      <td>0.354599</td>\n",
       "      <td>0.470828</td>\n",
       "      <td>0.354599</td>\n",
       "      <td>0.584456</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.088243</td>\n",
       "      <td>0.227739</td>\n",
       "      <td>0.088243</td>\n",
       "      <td>0.913449</td>\n",
       "      <td>0.390114</td>\n",
       "      <td>0.477033</td>\n",
       "      <td>0.390114</td>\n",
       "      <td>0.576166</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.086861</td>\n",
       "      <td>0.226444</td>\n",
       "      <td>0.086861</td>\n",
       "      <td>0.912672</td>\n",
       "      <td>0.386813</td>\n",
       "      <td>0.487267</td>\n",
       "      <td>0.386813</td>\n",
       "      <td>0.582383</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.085930</td>\n",
       "      <td>0.219982</td>\n",
       "      <td>0.085930</td>\n",
       "      <td>0.917077</td>\n",
       "      <td>0.373921</td>\n",
       "      <td>0.475716</td>\n",
       "      <td>0.373921</td>\n",
       "      <td>0.579275</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.100552</td>\n",
       "      <td>0.224247</td>\n",
       "      <td>0.100552</td>\n",
       "      <td>0.915522</td>\n",
       "      <td>0.377266</td>\n",
       "      <td>0.475867</td>\n",
       "      <td>0.377266</td>\n",
       "      <td>0.586528</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.217271</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.922001</td>\n",
       "      <td>0.368029</td>\n",
       "      <td>0.472231</td>\n",
       "      <td>0.368029</td>\n",
       "      <td>0.592746</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.081671</td>\n",
       "      <td>0.213886</td>\n",
       "      <td>0.081671</td>\n",
       "      <td>0.926147</td>\n",
       "      <td>0.401092</td>\n",
       "      <td>0.489022</td>\n",
       "      <td>0.401092</td>\n",
       "      <td>0.578238</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.077644</td>\n",
       "      <td>0.213379</td>\n",
       "      <td>0.077644</td>\n",
       "      <td>0.926924</td>\n",
       "      <td>0.383546</td>\n",
       "      <td>0.482525</td>\n",
       "      <td>0.383546</td>\n",
       "      <td>0.589637</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.076778</td>\n",
       "      <td>0.210751</td>\n",
       "      <td>0.076778</td>\n",
       "      <td>0.919409</td>\n",
       "      <td>0.375302</td>\n",
       "      <td>0.480297</td>\n",
       "      <td>0.375302</td>\n",
       "      <td>0.583420</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.074912</td>\n",
       "      <td>0.208784</td>\n",
       "      <td>0.074912</td>\n",
       "      <td>0.932366</td>\n",
       "      <td>0.380694</td>\n",
       "      <td>0.484055</td>\n",
       "      <td>0.380694</td>\n",
       "      <td>0.573057</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.076321</td>\n",
       "      <td>0.211341</td>\n",
       "      <td>0.076321</td>\n",
       "      <td>0.924851</td>\n",
       "      <td>0.387844</td>\n",
       "      <td>0.483286</td>\n",
       "      <td>0.387844</td>\n",
       "      <td>0.596891</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.078008</td>\n",
       "      <td>0.210235</td>\n",
       "      <td>0.078008</td>\n",
       "      <td>0.929515</td>\n",
       "      <td>0.407709</td>\n",
       "      <td>0.497036</td>\n",
       "      <td>0.407709</td>\n",
       "      <td>0.577202</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.076102</td>\n",
       "      <td>0.209703</td>\n",
       "      <td>0.076102</td>\n",
       "      <td>0.933143</td>\n",
       "      <td>0.378735</td>\n",
       "      <td>0.481845</td>\n",
       "      <td>0.378735</td>\n",
       "      <td>0.598964</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.076015</td>\n",
       "      <td>0.211742</td>\n",
       "      <td>0.076015</td>\n",
       "      <td>0.928220</td>\n",
       "      <td>0.366914</td>\n",
       "      <td>0.483156</td>\n",
       "      <td>0.366914</td>\n",
       "      <td>0.587565</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.205903</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.931070</td>\n",
       "      <td>0.412369</td>\n",
       "      <td>0.494609</td>\n",
       "      <td>0.412369</td>\n",
       "      <td>0.582383</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.075107</td>\n",
       "      <td>0.206152</td>\n",
       "      <td>0.075107</td>\n",
       "      <td>0.931588</td>\n",
       "      <td>0.393276</td>\n",
       "      <td>0.479850</td>\n",
       "      <td>0.393276</td>\n",
       "      <td>0.587565</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.070223</td>\n",
       "      <td>0.202889</td>\n",
       "      <td>0.070223</td>\n",
       "      <td>0.935994</td>\n",
       "      <td>0.388184</td>\n",
       "      <td>0.489135</td>\n",
       "      <td>0.388184</td>\n",
       "      <td>0.588601</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.069491</td>\n",
       "      <td>0.196689</td>\n",
       "      <td>0.069491</td>\n",
       "      <td>0.939103</td>\n",
       "      <td>0.390127</td>\n",
       "      <td>0.489278</td>\n",
       "      <td>0.390127</td>\n",
       "      <td>0.582383</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.072107</td>\n",
       "      <td>0.203260</td>\n",
       "      <td>0.072107</td>\n",
       "      <td>0.934957</td>\n",
       "      <td>0.424696</td>\n",
       "      <td>0.497504</td>\n",
       "      <td>0.424696</td>\n",
       "      <td>0.577202</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.068870</td>\n",
       "      <td>0.195689</td>\n",
       "      <td>0.068870</td>\n",
       "      <td>0.937549</td>\n",
       "      <td>0.399479</td>\n",
       "      <td>0.480540</td>\n",
       "      <td>0.399479</td>\n",
       "      <td>0.598964</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       mae       mse  accuracy  val_loss   val_mae   val_mse  \\\n",
       "0   0.520347  0.552337  0.520347  0.502462  0.316236  0.501488  0.316236   \n",
       "1   0.454838  0.495181  0.454838  0.584348  0.322664  0.491624  0.322664   \n",
       "2   0.695899  0.498929  0.695899  0.600933  0.406425  0.490341  0.406425   \n",
       "3   0.444578  0.466476  0.444578  0.644467  0.250883  0.456547  0.250883   \n",
       "4   0.302687  0.444945  0.302687  0.670899  0.286613  0.472490  0.286613   \n",
       "5   0.265774  0.431594  0.265774  0.683856  0.249688  0.451508  0.249688   \n",
       "6   0.265209  0.420058  0.265209  0.698886  0.252444  0.457889  0.252444   \n",
       "7   0.261960  0.420434  0.261960  0.703809  0.260153  0.440372  0.260153   \n",
       "8   0.243129  0.411148  0.243129  0.706919  0.241719  0.443542  0.241719   \n",
       "9   0.199984  0.395529  0.199984  0.718839  0.244681  0.438630  0.244681   \n",
       "10  0.185535  0.385836  0.185535  0.734646  0.241440  0.431843  0.241440   \n",
       "11  0.178355  0.377364  0.178355  0.734387  0.247518  0.435035  0.247518   \n",
       "12  0.174848  0.371667  0.174848  0.739311  0.257204  0.437038  0.257204   \n",
       "13  0.172409  0.367592  0.172409  0.749935  0.248361  0.432193  0.248361   \n",
       "14  0.171032  0.365651  0.171032  0.752008  0.262475  0.432861  0.262475   \n",
       "15  0.172953  0.363855  0.172953  0.751749  0.261119  0.430813  0.261119   \n",
       "16  0.174292  0.362078  0.174292  0.752008  0.265139  0.437718  0.265139   \n",
       "17  0.176555  0.364518  0.176555  0.753045  0.263109  0.436768  0.263109   \n",
       "18  0.178370  0.362813  0.178370  0.763669  0.272346  0.437083  0.272346   \n",
       "19  0.184988  0.362001  0.184988  0.769889  0.283589  0.446760  0.283589   \n",
       "20  0.187240  0.355876  0.187240  0.773776  0.271826  0.433598  0.271826   \n",
       "21  0.182398  0.351863  0.182397  0.767297  0.282528  0.438621  0.282528   \n",
       "22  0.213529  0.357452  0.213529  0.770148  0.284735  0.442502  0.284735   \n",
       "23  0.221026  0.347171  0.221026  0.780772  0.284410  0.443545  0.284410   \n",
       "24  0.176284  0.343768  0.176284  0.782068  0.282924  0.441076  0.282924   \n",
       "25  0.171933  0.336655  0.171933  0.788805  0.278783  0.440627  0.278783   \n",
       "26  0.155029  0.332139  0.155029  0.789842  0.275450  0.437253  0.275450   \n",
       "27  0.151617  0.326396  0.151617  0.800207  0.290119  0.442603  0.290119   \n",
       "28  0.146623  0.323706  0.146623  0.797875  0.293054  0.441134  0.293054   \n",
       "29  0.145412  0.321068  0.145412  0.800726  0.280624  0.429089  0.280624   \n",
       "30  0.146809  0.319062  0.146809  0.798134  0.304120  0.442576  0.304120   \n",
       "31  0.142902  0.317803  0.142902  0.808500  0.305393  0.452055  0.305393   \n",
       "32  0.147912  0.319935  0.147912  0.805131  0.311074  0.452209  0.311074   \n",
       "33  0.145579  0.319990  0.145579  0.811091  0.304765  0.440190  0.304765   \n",
       "34  0.145954  0.317363  0.145954  0.812127  0.304182  0.447471  0.304182   \n",
       "35  0.142400  0.311979  0.142400  0.810832  0.304576  0.445671  0.304576   \n",
       "36  0.138166  0.307870  0.138166  0.818606  0.294179  0.443598  0.294179   \n",
       "37  0.133141  0.301944  0.133141  0.825343  0.285918  0.433358  0.285918   \n",
       "38  0.134610  0.300304  0.134610  0.828453  0.346460  0.457151  0.346460   \n",
       "39  0.138952  0.302036  0.138952  0.827157  0.290811  0.440973  0.290811   \n",
       "40  0.139080  0.304806  0.139080  0.828453  0.298607  0.437821  0.298607   \n",
       "41  0.131902  0.296817  0.131902  0.831563  0.312819  0.450928  0.312819   \n",
       "42  0.128360  0.292820  0.128360  0.838818  0.308396  0.449673  0.308396   \n",
       "43  0.123467  0.287456  0.123467  0.845556  0.321950  0.456315  0.321950   \n",
       "44  0.122169  0.286573  0.122169  0.846852  0.318115  0.449507  0.318115   \n",
       "45  0.126213  0.285741  0.126213  0.848925  0.312965  0.443511  0.312965   \n",
       "46  0.130984  0.290656  0.130984  0.844519  0.296398  0.455058  0.296398   \n",
       "47  0.137016  0.294774  0.137016  0.841410  0.313312  0.452519  0.313312   \n",
       "48  0.134573  0.292788  0.134572  0.841151  0.301042  0.445547  0.301042   \n",
       "49  0.165503  0.287472  0.165503  0.847111  0.327389  0.457005  0.327389   \n",
       "50  0.126144  0.284184  0.126144  0.851257  0.312080  0.455250  0.312080   \n",
       "51  0.155782  0.283805  0.155782  0.855662  0.315460  0.461231  0.315460   \n",
       "52  0.128029  0.277409  0.128029  0.866805  0.315137  0.463065  0.315137   \n",
       "53  0.117441  0.275244  0.117441  0.864991  0.320473  0.461049  0.320473   \n",
       "54  0.114263  0.271744  0.114263  0.867064  0.324848  0.461447  0.324848   \n",
       "55  0.113146  0.267397  0.113146  0.868619  0.326166  0.454829  0.326166   \n",
       "56  0.117217  0.269175  0.117217  0.870433  0.349644  0.467936  0.349644   \n",
       "57  0.147392  0.277821  0.147392  0.865768  0.407535  0.475248  0.407535   \n",
       "58  0.115787  0.264909  0.115787  0.878207  0.322683  0.456025  0.322683   \n",
       "59  0.113452  0.264480  0.113452  0.873283  0.337800  0.457973  0.337800   \n",
       "60  0.108656  0.262339  0.108656  0.871210  0.326107  0.465386  0.326107   \n",
       "61  0.101510  0.253828  0.101510  0.886240  0.335299  0.466900  0.335299   \n",
       "62  0.101363  0.252191  0.101363  0.884426  0.346949  0.463975  0.346949   \n",
       "63  0.102374  0.250274  0.102374  0.883908  0.342197  0.465983  0.342197   \n",
       "64  0.100814  0.251374  0.100814  0.890127  0.330664  0.456036  0.330664   \n",
       "65  0.097096  0.246530  0.097096  0.888313  0.335145  0.464958  0.335145   \n",
       "66  0.094367  0.241723  0.094367  0.896087  0.348889  0.467693  0.348889   \n",
       "67  0.096191  0.245583  0.096191  0.895310  0.353573  0.471152  0.353573   \n",
       "68  0.100730  0.246261  0.100730  0.893237  0.347921  0.469966  0.347921   \n",
       "69  0.108966  0.252820  0.108966  0.886499  0.370134  0.475845  0.370134   \n",
       "70  0.111371  0.255682  0.111371  0.885203  0.371120  0.482183  0.371120   \n",
       "71  0.102001  0.246493  0.102001  0.889350  0.382253  0.480616  0.382253   \n",
       "72  0.095462  0.243538  0.095462  0.894014  0.356529  0.476565  0.356529   \n",
       "73  0.090229  0.235639  0.090229  0.907230  0.381325  0.480067  0.381325   \n",
       "74  0.089881  0.232765  0.089881  0.905934  0.360463  0.478798  0.360463   \n",
       "75  0.088535  0.231630  0.088535  0.907230  0.373845  0.484971  0.373845   \n",
       "76  0.086556  0.228263  0.086556  0.909562  0.361072  0.476318  0.361072   \n",
       "77  0.085380  0.228040  0.085380  0.909562  0.372852  0.476610  0.372852   \n",
       "78  0.082005  0.222452  0.082005  0.913708  0.377784  0.479639  0.377784   \n",
       "79  0.085173  0.224505  0.085173  0.914486  0.368166  0.479887  0.368166   \n",
       "80  0.086452  0.223505  0.086452  0.908007  0.354599  0.470828  0.354599   \n",
       "81  0.088243  0.227739  0.088243  0.913449  0.390114  0.477033  0.390114   \n",
       "82  0.086861  0.226444  0.086861  0.912672  0.386813  0.487267  0.386813   \n",
       "83  0.085930  0.219982  0.085930  0.917077  0.373921  0.475716  0.373921   \n",
       "84  0.100552  0.224247  0.100552  0.915522  0.377266  0.475867  0.377266   \n",
       "85  0.081800  0.217271  0.081800  0.922001  0.368029  0.472231  0.368029   \n",
       "86  0.081671  0.213886  0.081671  0.926147  0.401092  0.489022  0.401092   \n",
       "87  0.077644  0.213379  0.077644  0.926924  0.383546  0.482525  0.383546   \n",
       "88  0.076778  0.210751  0.076778  0.919409  0.375302  0.480297  0.375302   \n",
       "89  0.074912  0.208784  0.074912  0.932366  0.380694  0.484055  0.380694   \n",
       "90  0.076321  0.211341  0.076321  0.924851  0.387844  0.483286  0.387844   \n",
       "91  0.078008  0.210235  0.078008  0.929515  0.407709  0.497036  0.407709   \n",
       "92  0.076102  0.209703  0.076102  0.933143  0.378735  0.481845  0.378735   \n",
       "93  0.076015  0.211742  0.076015  0.928220  0.366914  0.483156  0.366914   \n",
       "94  0.073333  0.205903  0.073333  0.931070  0.412369  0.494609  0.412369   \n",
       "95  0.075107  0.206152  0.075107  0.931588  0.393276  0.479850  0.393276   \n",
       "96  0.070223  0.202889  0.070223  0.935994  0.388184  0.489135  0.388184   \n",
       "97  0.069491  0.196689  0.069491  0.939103  0.390127  0.489278  0.390127   \n",
       "98  0.072107  0.203260  0.072107  0.934957  0.424696  0.497504  0.424696   \n",
       "99  0.068870  0.195689  0.068870  0.937549  0.399479  0.480540  0.399479   \n",
       "\n",
       "    val_accuracy  epoch  \n",
       "0   0.538860      0      \n",
       "1   0.583420      1      \n",
       "2   0.604145      2      \n",
       "3   0.619689      3      \n",
       "4   0.622798      4      \n",
       "5   0.644560      5      \n",
       "6   0.646632      6      \n",
       "7   0.638342      7      \n",
       "8   0.640415      8      \n",
       "9   0.663212      9      \n",
       "10  0.647668      10     \n",
       "11  0.654922      11     \n",
       "12  0.645596      12     \n",
       "13  0.653886      13     \n",
       "14  0.658031      14     \n",
       "15  0.653886      15     \n",
       "16  0.641451      16     \n",
       "17  0.641451      17     \n",
       "18  0.640415      18     \n",
       "19  0.632124      19     \n",
       "20  0.649741      20     \n",
       "21  0.633161      21     \n",
       "22  0.635233      22     \n",
       "23  0.623834      23     \n",
       "24  0.642487      24     \n",
       "25  0.647668      25     \n",
       "26  0.636269      26     \n",
       "27  0.637306      27     \n",
       "28  0.631088      28     \n",
       "29  0.640415      29     \n",
       "30  0.635233      30     \n",
       "31  0.606218      31     \n",
       "32  0.619689      32     \n",
       "33  0.627979      33     \n",
       "34  0.618653      34     \n",
       "35  0.625907      35     \n",
       "36  0.616580      36     \n",
       "37  0.631088      37     \n",
       "38  0.615544      38     \n",
       "39  0.627979      39     \n",
       "40  0.622798      40     \n",
       "41  0.608290      41     \n",
       "42  0.609326      42     \n",
       "43  0.600000      43     \n",
       "44  0.616580      44     \n",
       "45  0.617617      45     \n",
       "46  0.600000      46     \n",
       "47  0.602073      47     \n",
       "48  0.610363      48     \n",
       "49  0.591710      49     \n",
       "50  0.593782      50     \n",
       "51  0.587565      51     \n",
       "52  0.609326      52     \n",
       "53  0.607254      53     \n",
       "54  0.593782      54     \n",
       "55  0.606218      55     \n",
       "56  0.582383      56     \n",
       "57  0.586528      57     \n",
       "58  0.589637      58     \n",
       "59  0.595855      59     \n",
       "60  0.589637      60     \n",
       "61  0.574093      61     \n",
       "62  0.585492      62     \n",
       "63  0.575130      63     \n",
       "64  0.588601      64     \n",
       "65  0.588601      65     \n",
       "66  0.591710      66     \n",
       "67  0.585492      67     \n",
       "68  0.597927      68     \n",
       "69  0.598964      69     \n",
       "70  0.577202      70     \n",
       "71  0.589637      71     \n",
       "72  0.591710      72     \n",
       "73  0.579275      73     \n",
       "74  0.582383      74     \n",
       "75  0.564767      75     \n",
       "76  0.600000      76     \n",
       "77  0.581347      77     \n",
       "78  0.594819      78     \n",
       "79  0.579275      79     \n",
       "80  0.584456      80     \n",
       "81  0.576166      81     \n",
       "82  0.582383      82     \n",
       "83  0.579275      83     \n",
       "84  0.586528      84     \n",
       "85  0.592746      85     \n",
       "86  0.578238      86     \n",
       "87  0.589637      87     \n",
       "88  0.583420      88     \n",
       "89  0.573057      89     \n",
       "90  0.596891      90     \n",
       "91  0.577202      91     \n",
       "92  0.598964      92     \n",
       "93  0.587565      93     \n",
       "94  0.582383      94     \n",
       "95  0.587565      95     \n",
       "96  0.588601      96     \n",
       "97  0.582383      97     \n",
       "98  0.577202      98     \n",
       "99  0.598964      99     "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4824 samples\n",
      "Epoch 1/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0725 - mae: 0.1510 - mse: 0.0725 - accuracy: 0.9760\n",
      "Epoch 2/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.1135 - mae: 0.1504 - mse: 0.1135 - accuracy: 0.9768\n",
      "Epoch 3/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0392 - mae: 0.1467 - mse: 0.0392 - accuracy: 0.9768\n",
      "Epoch 4/200\n",
      "4824/4824 [==============================] - 0s 39us/sample - loss: 0.0362 - mae: 0.1401 - mse: 0.0362 - accuracy: 0.9801\n",
      "Epoch 5/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0341 - mae: 0.1349 - mse: 0.0341 - accuracy: 0.9813\n",
      "Epoch 6/200\n",
      "4824/4824 [==============================] - 0s 39us/sample - loss: 0.0356 - mae: 0.1378 - mse: 0.0356 - accuracy: 0.9820\n",
      "Epoch 7/200\n",
      "4824/4824 [==============================] - 0s 40us/sample - loss: 0.0404 - mae: 0.1397 - mse: 0.0404 - accuracy: 0.9795\n",
      "Epoch 8/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0416 - mae: 0.1480 - mse: 0.0416 - accuracy: 0.9782\n",
      "Epoch 9/200\n",
      "4824/4824 [==============================] - 0s 40us/sample - loss: 0.0364 - mae: 0.1395 - mse: 0.0364 - accuracy: 0.9778\n",
      "Epoch 10/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0393 - mae: 0.1469 - mse: 0.0393 - accuracy: 0.9780\n",
      "Epoch 11/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0346 - mae: 0.1372 - mse: 0.0346 - accuracy: 0.9822\n",
      "Epoch 12/200\n",
      "4824/4824 [==============================] - 0s 44us/sample - loss: 0.0341 - mae: 0.1352 - mse: 0.0341 - accuracy: 0.9807\n",
      "Epoch 13/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0338 - mae: 0.1346 - mse: 0.0338 - accuracy: 0.9813\n",
      "Epoch 14/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0329 - mae: 0.1331 - mse: 0.0329 - accuracy: 0.9832\n",
      "Epoch 15/200\n",
      "4824/4824 [==============================] - 0s 40us/sample - loss: 0.0344 - mae: 0.1359 - mse: 0.0344 - accuracy: 0.9818\n",
      "Epoch 16/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0368 - mae: 0.1376 - mse: 0.0368 - accuracy: 0.9822\n",
      "Epoch 17/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0334 - mae: 0.1347 - mse: 0.0334 - accuracy: 0.9822\n",
      "Epoch 18/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0340 - mae: 0.1361 - mse: 0.0340 - accuracy: 0.9818\n",
      "Epoch 19/200\n",
      "4824/4824 [==============================] - 0s 35us/sample - loss: 0.0317 - mae: 0.1291 - mse: 0.0317 - accuracy: 0.9838\n",
      "Epoch 20/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0307 - mae: 0.1283 - mse: 0.0307 - accuracy: 0.9853\n",
      "Epoch 21/200\n",
      "4824/4824 [==============================] - 0s 41us/sample - loss: 0.0329 - mae: 0.1324 - mse: 0.0329 - accuracy: 0.9822\n",
      "Epoch 22/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0307 - mae: 0.1279 - mse: 0.0307 - accuracy: 0.9849\n",
      "Epoch 23/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0327 - mae: 0.1330 - mse: 0.0327 - accuracy: 0.9842\n",
      "Epoch 24/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0326 - mae: 0.1320 - mse: 0.0326 - accuracy: 0.9845\n",
      "Epoch 25/200\n",
      "4824/4824 [==============================] - 0s 41us/sample - loss: 0.0332 - mae: 0.1337 - mse: 0.0332 - accuracy: 0.9822\n",
      "Epoch 26/200\n",
      "4824/4824 [==============================] - 0s 45us/sample - loss: 0.0323 - mae: 0.1315 - mse: 0.0323 - accuracy: 0.9851\n",
      "Epoch 27/200\n",
      "4824/4824 [==============================] - 0s 44us/sample - loss: 0.0346 - mae: 0.1372 - mse: 0.0346 - accuracy: 0.9836\n",
      "Epoch 28/200\n",
      "4824/4824 [==============================] - 0s 49us/sample - loss: 0.0313 - mae: 0.1300 - mse: 0.0313 - accuracy: 0.9845\n",
      "Epoch 29/200\n",
      "4824/4824 [==============================] - 0s 43us/sample - loss: 0.0330 - mae: 0.1319 - mse: 0.0330 - accuracy: 0.9818\n",
      "Epoch 30/200\n",
      "4824/4824 [==============================] - 0s 43us/sample - loss: 0.0310 - mae: 0.1278 - mse: 0.0310 - accuracy: 0.9867\n",
      "Epoch 31/200\n",
      "4824/4824 [==============================] - 0s 40us/sample - loss: 0.0306 - mae: 0.1287 - mse: 0.0306 - accuracy: 0.9859\n",
      "Epoch 32/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0294 - mae: 0.1255 - mse: 0.0294 - accuracy: 0.9853\n",
      "Epoch 33/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0294 - mae: 0.1251 - mse: 0.0294 - accuracy: 0.9855\n",
      "Epoch 34/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0317 - mae: 0.1299 - mse: 0.0317 - accuracy: 0.9851\n",
      "Epoch 35/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0307 - mae: 0.1269 - mse: 0.0307 - accuracy: 0.9863\n",
      "Epoch 36/200\n",
      "4824/4824 [==============================] - 0s 41us/sample - loss: 0.0291 - mae: 0.1246 - mse: 0.0291 - accuracy: 0.9876\n",
      "Epoch 37/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0297 - mae: 0.1242 - mse: 0.0297 - accuracy: 0.9845\n",
      "Epoch 38/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0283 - mae: 0.1219 - mse: 0.0283 - accuracy: 0.9865\n",
      "Epoch 39/200\n",
      "4824/4824 [==============================] - 0s 39us/sample - loss: 0.0280 - mae: 0.1233 - mse: 0.0280 - accuracy: 0.9886\n",
      "Epoch 40/200\n",
      "4824/4824 [==============================] - 0s 35us/sample - loss: 0.0295 - mae: 0.1275 - mse: 0.0295 - accuracy: 0.9878\n",
      "Epoch 41/200\n",
      "4824/4824 [==============================] - 0s 40us/sample - loss: 0.0292 - mae: 0.1261 - mse: 0.0292 - accuracy: 0.9876\n",
      "Epoch 42/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0285 - mae: 0.1246 - mse: 0.0285 - accuracy: 0.9880\n",
      "Epoch 43/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0291 - mae: 0.1253 - mse: 0.0291 - accuracy: 0.9880\n",
      "Epoch 44/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0300 - mae: 0.1250 - mse: 0.0300 - accuracy: 0.9878\n",
      "Epoch 45/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0291 - mae: 0.1244 - mse: 0.0291 - accuracy: 0.9855\n",
      "Epoch 46/200\n",
      "4824/4824 [==============================] - 0s 40us/sample - loss: 0.0286 - mae: 0.1221 - mse: 0.0286 - accuracy: 0.9869\n",
      "Epoch 47/200\n",
      "4824/4824 [==============================] - 0s 41us/sample - loss: 0.0346 - mae: 0.1233 - mse: 0.0346 - accuracy: 0.9886\n",
      "Epoch 48/200\n",
      "4824/4824 [==============================] - 0s 41us/sample - loss: 0.0283 - mae: 0.1240 - mse: 0.0283 - accuracy: 0.9892\n",
      "Epoch 49/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0259 - mae: 0.1176 - mse: 0.0259 - accuracy: 0.9898\n",
      "Epoch 50/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0256 - mae: 0.1159 - mse: 0.0256 - accuracy: 0.9907\n",
      "Epoch 51/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0270 - mae: 0.1209 - mse: 0.0270 - accuracy: 0.9894\n",
      "Epoch 52/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0266 - mae: 0.1201 - mse: 0.0266 - accuracy: 0.9880\n",
      "Epoch 53/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0269 - mae: 0.1204 - mse: 0.0269 - accuracy: 0.9886\n",
      "Epoch 54/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0277 - mae: 0.1211 - mse: 0.0277 - accuracy: 0.9867\n",
      "Epoch 55/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0274 - mae: 0.1207 - mse: 0.0274 - accuracy: 0.9896\n",
      "Epoch 56/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0251 - mae: 0.1151 - mse: 0.0251 - accuracy: 0.9909\n",
      "Epoch 57/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0250 - mae: 0.1144 - mse: 0.0250 - accuracy: 0.9896\n",
      "Epoch 58/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0263 - mae: 0.1186 - mse: 0.0263 - accuracy: 0.9898\n",
      "Epoch 59/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0269 - mae: 0.1177 - mse: 0.0269 - accuracy: 0.9888\n",
      "Epoch 60/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0271 - mae: 0.1194 - mse: 0.0271 - accuracy: 0.9892\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0292 - mae: 0.1217 - mse: 0.0292 - accuracy: 0.9878\n",
      "Epoch 62/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0275 - mae: 0.1198 - mse: 0.0275 - accuracy: 0.9886\n",
      "Epoch 63/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0255 - mae: 0.1155 - mse: 0.0255 - accuracy: 0.9890\n",
      "Epoch 64/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0243 - mae: 0.1137 - mse: 0.0243 - accuracy: 0.9905\n",
      "Epoch 65/200\n",
      "4824/4824 [==============================] - 0s 41us/sample - loss: 0.0263 - mae: 0.1139 - mse: 0.0263 - accuracy: 0.9909\n",
      "Epoch 66/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0243 - mae: 0.1148 - mse: 0.0243 - accuracy: 0.9913\n",
      "Epoch 67/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0234 - mae: 0.1110 - mse: 0.0234 - accuracy: 0.9917\n",
      "Epoch 68/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0244 - mae: 0.1147 - mse: 0.0244 - accuracy: 0.9898\n",
      "Epoch 69/200\n",
      "4824/4824 [==============================] - 0s 39us/sample - loss: 0.0311 - mae: 0.1167 - mse: 0.0311 - accuracy: 0.9903\n",
      "Epoch 70/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0257 - mae: 0.1147 - mse: 0.0257 - accuracy: 0.9915\n",
      "Epoch 71/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0246 - mae: 0.1151 - mse: 0.0246 - accuracy: 0.9907\n",
      "Epoch 72/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0247 - mae: 0.1156 - mse: 0.0247 - accuracy: 0.9907\n",
      "Epoch 73/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0232 - mae: 0.1104 - mse: 0.0232 - accuracy: 0.9917\n",
      "Epoch 74/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0234 - mae: 0.1121 - mse: 0.0234 - accuracy: 0.9917\n",
      "Epoch 75/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0233 - mae: 0.1095 - mse: 0.0233 - accuracy: 0.9898\n",
      "Epoch 76/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0228 - mae: 0.1100 - mse: 0.0228 - accuracy: 0.9917\n",
      "Epoch 77/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0233 - mae: 0.1105 - mse: 0.0233 - accuracy: 0.9913\n",
      "Epoch 78/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0226 - mae: 0.1094 - mse: 0.0226 - accuracy: 0.9919\n",
      "Epoch 79/200\n",
      "4824/4824 [==============================] - 0s 39us/sample - loss: 0.0235 - mae: 0.1123 - mse: 0.0235 - accuracy: 0.9923\n",
      "Epoch 80/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0228 - mae: 0.1101 - mse: 0.0228 - accuracy: 0.9927\n",
      "Epoch 81/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0219 - mae: 0.1074 - mse: 0.0219 - accuracy: 0.9930\n",
      "Epoch 82/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0237 - mae: 0.1093 - mse: 0.0237 - accuracy: 0.9905\n",
      "Epoch 83/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0226 - mae: 0.1102 - mse: 0.0226 - accuracy: 0.9932\n",
      "Epoch 84/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0213 - mae: 0.1062 - mse: 0.0213 - accuracy: 0.9942\n",
      "Epoch 85/200\n",
      "4824/4824 [==============================] - 0s 40us/sample - loss: 0.0231 - mae: 0.1109 - mse: 0.0231 - accuracy: 0.9917\n",
      "Epoch 86/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0230 - mae: 0.1092 - mse: 0.0230 - accuracy: 0.9917\n",
      "Epoch 87/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0222 - mae: 0.1087 - mse: 0.0222 - accuracy: 0.9917\n",
      "Epoch 88/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0231 - mae: 0.1110 - mse: 0.0231 - accuracy: 0.9915\n",
      "Epoch 89/200\n",
      "4824/4824 [==============================] - 0s 35us/sample - loss: 0.0224 - mae: 0.1097 - mse: 0.0224 - accuracy: 0.9927\n",
      "Epoch 90/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0221 - mae: 0.1076 - mse: 0.0221 - accuracy: 0.9913\n",
      "Epoch 91/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0221 - mae: 0.1089 - mse: 0.0221 - accuracy: 0.9934\n",
      "Epoch 92/200\n",
      "4824/4824 [==============================] - 0s 35us/sample - loss: 0.0206 - mae: 0.1050 - mse: 0.0206 - accuracy: 0.9934\n",
      "Epoch 93/200\n",
      "4824/4824 [==============================] - 0s 35us/sample - loss: 0.0205 - mae: 0.1030 - mse: 0.0205 - accuracy: 0.9925\n",
      "Epoch 94/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0199 - mae: 0.1026 - mse: 0.0199 - accuracy: 0.9940\n",
      "Epoch 95/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0213 - mae: 0.1058 - mse: 0.0213 - accuracy: 0.9932\n",
      "Epoch 96/200\n",
      "4824/4824 [==============================] - 0s 39us/sample - loss: 0.0218 - mae: 0.1085 - mse: 0.0218 - accuracy: 0.9950\n",
      "Epoch 97/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0225 - mae: 0.1101 - mse: 0.0225 - accuracy: 0.9940\n",
      "Epoch 98/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0240 - mae: 0.1129 - mse: 0.0240 - accuracy: 0.9923\n",
      "Epoch 99/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0235 - mae: 0.1121 - mse: 0.0235 - accuracy: 0.9923\n",
      "Epoch 100/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0215 - mae: 0.1065 - mse: 0.0215 - accuracy: 0.9930\n",
      "Epoch 101/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0192 - mae: 0.1014 - mse: 0.0192 - accuracy: 0.9946\n",
      "Epoch 102/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0189 - mae: 0.0996 - mse: 0.0189 - accuracy: 0.9950\n",
      "Epoch 103/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0193 - mae: 0.1006 - mse: 0.0193 - accuracy: 0.9944\n",
      "Epoch 104/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0191 - mae: 0.1009 - mse: 0.0191 - accuracy: 0.9952\n",
      "Epoch 105/200\n",
      "4824/4824 [==============================] - 0s 35us/sample - loss: 0.0192 - mae: 0.1005 - mse: 0.0192 - accuracy: 0.9948\n",
      "Epoch 106/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0211 - mae: 0.1062 - mse: 0.0211 - accuracy: 0.9942\n",
      "Epoch 107/200\n",
      "4824/4824 [==============================] - 0s 41us/sample - loss: 0.0224 - mae: 0.1078 - mse: 0.0224 - accuracy: 0.9932\n",
      "Epoch 108/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0214 - mae: 0.1059 - mse: 0.0214 - accuracy: 0.9938\n",
      "Epoch 109/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0206 - mae: 0.1042 - mse: 0.0206 - accuracy: 0.9925\n",
      "Epoch 110/200\n",
      "4824/4824 [==============================] - 0s 51us/sample - loss: 0.0191 - mae: 0.1005 - mse: 0.0191 - accuracy: 0.9936\n",
      "Epoch 111/200\n",
      "4824/4824 [==============================] - 0s 43us/sample - loss: 0.0192 - mae: 0.1005 - mse: 0.0192 - accuracy: 0.9936\n",
      "Epoch 112/200\n",
      "4824/4824 [==============================] - 0s 48us/sample - loss: 0.0184 - mae: 0.0971 - mse: 0.0184 - accuracy: 0.9940\n",
      "Epoch 113/200\n",
      "4824/4824 [==============================] - 0s 42us/sample - loss: 0.0195 - mae: 0.1010 - mse: 0.0195 - accuracy: 0.9946\n",
      "Epoch 114/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0207 - mae: 0.1031 - mse: 0.0207 - accuracy: 0.9919\n",
      "Epoch 115/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0201 - mae: 0.1019 - mse: 0.0201 - accuracy: 0.9940\n",
      "Epoch 116/200\n",
      "4824/4824 [==============================] - 0s 39us/sample - loss: 0.0189 - mae: 0.1004 - mse: 0.0189 - accuracy: 0.9954\n",
      "Epoch 117/200\n",
      "4824/4824 [==============================] - 0s 41us/sample - loss: 0.0202 - mae: 0.1031 - mse: 0.0202 - accuracy: 0.9930\n",
      "Epoch 118/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0202 - mae: 0.1037 - mse: 0.0202 - accuracy: 0.9938\n",
      "Epoch 119/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0199 - mae: 0.1028 - mse: 0.0199 - accuracy: 0.9942\n",
      "Epoch 120/200\n",
      "4824/4824 [==============================] - 0s 37us/sample - loss: 0.0193 - mae: 0.1007 - mse: 0.0193 - accuracy: 0.9942\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4824/4824 [==============================] - 0s 39us/sample - loss: 0.0182 - mae: 0.0986 - mse: 0.0182 - accuracy: 0.9959\n",
      "Epoch 122/200\n",
      "4824/4824 [==============================] - 0s 39us/sample - loss: 0.0189 - mae: 0.1009 - mse: 0.0189 - accuracy: 0.9952\n",
      "Epoch 123/200\n",
      "4824/4824 [==============================] - 0s 40us/sample - loss: 0.0179 - mae: 0.0974 - mse: 0.0179 - accuracy: 0.9946\n",
      "Epoch 124/200\n",
      "4824/4824 [==============================] - 0s 40us/sample - loss: 0.0179 - mae: 0.0973 - mse: 0.0179 - accuracy: 0.9950\n",
      "Epoch 125/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0202 - mae: 0.1022 - mse: 0.0202 - accuracy: 0.9930\n",
      "Epoch 126/200\n",
      "4824/4824 [==============================] - 0s 39us/sample - loss: 0.0193 - mae: 0.0994 - mse: 0.0193 - accuracy: 0.9946\n",
      "Epoch 127/200\n",
      "4824/4824 [==============================] - 0s 47us/sample - loss: 0.0219 - mae: 0.1079 - mse: 0.0219 - accuracy: 0.9936\n",
      "Epoch 128/200\n",
      "4824/4824 [==============================] - 0s 45us/sample - loss: 0.0196 - mae: 0.1016 - mse: 0.0196 - accuracy: 0.9942\n",
      "Epoch 129/200\n",
      "4824/4824 [==============================] - 0s 46us/sample - loss: 0.0182 - mae: 0.0977 - mse: 0.0182 - accuracy: 0.9934\n",
      "Epoch 130/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0165 - mae: 0.0939 - mse: 0.0165 - accuracy: 0.9961\n",
      "Epoch 131/200\n",
      "4824/4824 [==============================] - 0s 36us/sample - loss: 0.0189 - mae: 0.0990 - mse: 0.0189 - accuracy: 0.9967\n",
      "Epoch 132/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0181 - mae: 0.0981 - mse: 0.0181 - accuracy: 0.9950\n",
      "Epoch 133/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0177 - mae: 0.0971 - mse: 0.0177 - accuracy: 0.9950\n",
      "Epoch 134/200\n",
      "4824/4824 [==============================] - 0s 39us/sample - loss: 0.0174 - mae: 0.0951 - mse: 0.0174 - accuracy: 0.9954\n",
      "Epoch 135/200\n",
      "4824/4824 [==============================] - 0s 39us/sample - loss: 0.0185 - mae: 0.0980 - mse: 0.0185 - accuracy: 0.9952\n",
      "Epoch 136/200\n",
      "4824/4824 [==============================] - 0s 38us/sample - loss: 0.0202 - mae: 0.1037 - mse: 0.0202 - accuracy: 0.9954\n",
      "Epoch 137/200\n",
      "4824/4824 [==============================] - 0s 48us/sample - loss: 0.0187 - mae: 0.0995 - mse: 0.0187 - accuracy: 0.9952\n",
      "Epoch 138/200\n",
      "4824/4824 [==============================] - 0s 46us/sample - loss: 0.0183 - mae: 0.0994 - mse: 0.0183 - accuracy: 0.9967\n",
      "Epoch 139/200\n",
      "4824/4824 [==============================] - 0s 43us/sample - loss: 0.0173 - mae: 0.0965 - mse: 0.0173 - accuracy: 0.9961\n",
      "Epoch 140/200\n",
      "4824/4824 [==============================] - 0s 47us/sample - loss: 0.0171 - mae: 0.0948 - mse: 0.0171 - accuracy: 0.9954\n",
      "Epoch 141/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0171 - mae: 0.0945 - mse: 0.0171 - accuracy: 0.9952\n",
      "Epoch 142/200\n",
      "4824/4824 [==============================] - 0s 51us/sample - loss: 0.0174 - mae: 0.0958 - mse: 0.0174 - accuracy: 0.9956\n",
      "Epoch 143/200\n",
      "4824/4824 [==============================] - 0s 55us/sample - loss: 0.0179 - mae: 0.0974 - mse: 0.0179 - accuracy: 0.9965\n",
      "Epoch 144/200\n",
      "4824/4824 [==============================] - 0s 50us/sample - loss: 0.0174 - mae: 0.0949 - mse: 0.0174 - accuracy: 0.9954\n",
      "Epoch 145/200\n",
      "4824/4824 [==============================] - 0s 51us/sample - loss: 0.0182 - mae: 0.0967 - mse: 0.0182 - accuracy: 0.9944\n",
      "Epoch 146/200\n",
      "4824/4824 [==============================] - 0s 54us/sample - loss: 0.0176 - mae: 0.0957 - mse: 0.0176 - accuracy: 0.9950\n",
      "Epoch 147/200\n",
      "4824/4824 [==============================] - 0s 51us/sample - loss: 0.0164 - mae: 0.0914 - mse: 0.0164 - accuracy: 0.9959\n",
      "Epoch 148/200\n",
      "4824/4824 [==============================] - 0s 51us/sample - loss: 0.0166 - mae: 0.0937 - mse: 0.0166 - accuracy: 0.9963\n",
      "Epoch 149/200\n",
      "4824/4824 [==============================] - 0s 50us/sample - loss: 0.0166 - mae: 0.0945 - mse: 0.0166 - accuracy: 0.9969\n",
      "Epoch 150/200\n",
      "4824/4824 [==============================] - 0s 51us/sample - loss: 0.0161 - mae: 0.0927 - mse: 0.0161 - accuracy: 0.9961\n",
      "Epoch 151/200\n",
      "4824/4824 [==============================] - 0s 51us/sample - loss: 0.0179 - mae: 0.0966 - mse: 0.0179 - accuracy: 0.9954\n",
      "Epoch 152/200\n",
      "4824/4824 [==============================] - 0s 55us/sample - loss: 0.0169 - mae: 0.0942 - mse: 0.0169 - accuracy: 0.9952\n",
      "Epoch 153/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0168 - mae: 0.0944 - mse: 0.0168 - accuracy: 0.9959\n",
      "Epoch 154/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0160 - mae: 0.0922 - mse: 0.0160 - accuracy: 0.9963\n",
      "Epoch 155/200\n",
      "4824/4824 [==============================] - 0s 51us/sample - loss: 0.0169 - mae: 0.0945 - mse: 0.0169 - accuracy: 0.9963\n",
      "Epoch 156/200\n",
      "4824/4824 [==============================] - 0s 55us/sample - loss: 0.0191 - mae: 0.1002 - mse: 0.0191 - accuracy: 0.9946\n",
      "Epoch 157/200\n",
      "4824/4824 [==============================] - 0s 54us/sample - loss: 0.0172 - mae: 0.0956 - mse: 0.0172 - accuracy: 0.9969\n",
      "Epoch 158/200\n",
      "4824/4824 [==============================] - 0s 57us/sample - loss: 0.0167 - mae: 0.0934 - mse: 0.0167 - accuracy: 0.9952\n",
      "Epoch 159/200\n",
      "4824/4824 [==============================] - 0s 44us/sample - loss: 0.0170 - mae: 0.0937 - mse: 0.0170 - accuracy: 0.9959\n",
      "Epoch 160/200\n",
      "4824/4824 [==============================] - 0s 40us/sample - loss: 0.0144 - mae: 0.0868 - mse: 0.0144 - accuracy: 0.9967\n",
      "Epoch 161/200\n",
      "4824/4824 [==============================] - 0s 43us/sample - loss: 0.0152 - mae: 0.0886 - mse: 0.0152 - accuracy: 0.9969\n",
      "Epoch 162/200\n",
      "4824/4824 [==============================] - 0s 50us/sample - loss: 0.0149 - mae: 0.0887 - mse: 0.0149 - accuracy: 0.9959\n",
      "Epoch 163/200\n",
      "4824/4824 [==============================] - 0s 49us/sample - loss: 0.0155 - mae: 0.0898 - mse: 0.0155 - accuracy: 0.9967\n",
      "Epoch 164/200\n",
      "4824/4824 [==============================] - 0s 53us/sample - loss: 0.0168 - mae: 0.0945 - mse: 0.0168 - accuracy: 0.9959\n",
      "Epoch 165/200\n",
      "4824/4824 [==============================] - 0s 44us/sample - loss: 0.0166 - mae: 0.0935 - mse: 0.0166 - accuracy: 0.9956\n",
      "Epoch 166/200\n",
      "4824/4824 [==============================] - 0s 42us/sample - loss: 0.0151 - mae: 0.0893 - mse: 0.0151 - accuracy: 0.9969\n",
      "Epoch 167/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0162 - mae: 0.0932 - mse: 0.0162 - accuracy: 0.9963\n",
      "Epoch 168/200\n",
      "4824/4824 [==============================] - 0s 40us/sample - loss: 0.0154 - mae: 0.0906 - mse: 0.0154 - accuracy: 0.9971\n",
      "Epoch 169/200\n",
      "4824/4824 [==============================] - 0s 47us/sample - loss: 0.0150 - mae: 0.0888 - mse: 0.0150 - accuracy: 0.9965\n",
      "Epoch 170/200\n",
      "4824/4824 [==============================] - 0s 59us/sample - loss: 0.0151 - mae: 0.0883 - mse: 0.0151 - accuracy: 0.9961\n",
      "Epoch 171/200\n",
      "4824/4824 [==============================] - 0s 53us/sample - loss: 0.0176 - mae: 0.0976 - mse: 0.0176 - accuracy: 0.9948\n",
      "Epoch 172/200\n",
      "4824/4824 [==============================] - 0s 49us/sample - loss: 0.0162 - mae: 0.0930 - mse: 0.0162 - accuracy: 0.9952\n",
      "Epoch 173/200\n",
      "4824/4824 [==============================] - 0s 57us/sample - loss: 0.0149 - mae: 0.0884 - mse: 0.0149 - accuracy: 0.9969\n",
      "Epoch 174/200\n",
      "4824/4824 [==============================] - 0s 50us/sample - loss: 0.0172 - mae: 0.0959 - mse: 0.0172 - accuracy: 0.9961\n",
      "Epoch 175/200\n",
      "4824/4824 [==============================] - 0s 55us/sample - loss: 0.0164 - mae: 0.0924 - mse: 0.0164 - accuracy: 0.9963\n",
      "Epoch 176/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0155 - mae: 0.0892 - mse: 0.0155 - accuracy: 0.9971\n",
      "Epoch 177/200\n",
      "4824/4824 [==============================] - 0s 61us/sample - loss: 0.0159 - mae: 0.0899 - mse: 0.0159 - accuracy: 0.9959\n",
      "Epoch 178/200\n",
      "4824/4824 [==============================] - 0s 54us/sample - loss: 0.0146 - mae: 0.0876 - mse: 0.0146 - accuracy: 0.9965\n",
      "Epoch 179/200\n",
      "4824/4824 [==============================] - 0s 51us/sample - loss: 0.0150 - mae: 0.0880 - mse: 0.0150 - accuracy: 0.9963\n",
      "Epoch 180/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0151 - mae: 0.0883 - mse: 0.0151 - accuracy: 0.9965\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0150 - mae: 0.0894 - mse: 0.0150 - accuracy: 0.9975\n",
      "Epoch 182/200\n",
      "4824/4824 [==============================] - 0s 55us/sample - loss: 0.0155 - mae: 0.0906 - mse: 0.0155 - accuracy: 0.9965\n",
      "Epoch 183/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0153 - mae: 0.0902 - mse: 0.0153 - accuracy: 0.9959\n",
      "Epoch 184/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0160 - mae: 0.0920 - mse: 0.0160 - accuracy: 0.9969\n",
      "Epoch 185/200\n",
      "4824/4824 [==============================] - 0s 53us/sample - loss: 0.0185 - mae: 0.0990 - mse: 0.0185 - accuracy: 0.9946\n",
      "Epoch 186/200\n",
      "4824/4824 [==============================] - 0s 51us/sample - loss: 0.0180 - mae: 0.0965 - mse: 0.0180 - accuracy: 0.9954\n",
      "Epoch 187/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0154 - mae: 0.0901 - mse: 0.0154 - accuracy: 0.9965\n",
      "Epoch 188/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0150 - mae: 0.0888 - mse: 0.0150 - accuracy: 0.9959\n",
      "Epoch 189/200\n",
      "4824/4824 [==============================] - 0s 47us/sample - loss: 0.0149 - mae: 0.0872 - mse: 0.0149 - accuracy: 0.9965\n",
      "Epoch 190/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0149 - mae: 0.0886 - mse: 0.0149 - accuracy: 0.9973\n",
      "Epoch 191/200\n",
      "4824/4824 [==============================] - 0s 47us/sample - loss: 0.0141 - mae: 0.0857 - mse: 0.0141 - accuracy: 0.9965\n",
      "Epoch 192/200\n",
      "4824/4824 [==============================] - 0s 42us/sample - loss: 0.0143 - mae: 0.0869 - mse: 0.0143 - accuracy: 0.9967\n",
      "Epoch 193/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0159 - mae: 0.0908 - mse: 0.0159 - accuracy: 0.9973\n",
      "Epoch 194/200\n",
      "4824/4824 [==============================] - 0s 51us/sample - loss: 0.0136 - mae: 0.0842 - mse: 0.0136 - accuracy: 0.9973\n",
      "Epoch 195/200\n",
      "4824/4824 [==============================] - 0s 54us/sample - loss: 0.0157 - mae: 0.0900 - mse: 0.0157 - accuracy: 0.9971\n",
      "Epoch 196/200\n",
      "4824/4824 [==============================] - 0s 53us/sample - loss: 0.0155 - mae: 0.0896 - mse: 0.0155 - accuracy: 0.9977\n",
      "Epoch 197/200\n",
      "4824/4824 [==============================] - 0s 53us/sample - loss: 0.0152 - mae: 0.0891 - mse: 0.0152 - accuracy: 0.9956\n",
      "Epoch 198/200\n",
      "4824/4824 [==============================] - 0s 52us/sample - loss: 0.0142 - mae: 0.0852 - mse: 0.0142 - accuracy: 0.9969\n",
      "Epoch 199/200\n",
      "4824/4824 [==============================] - 0s 53us/sample - loss: 0.0169 - mae: 0.0945 - mse: 0.0169 - accuracy: 0.9950\n",
      "Epoch 200/200\n",
      "4824/4824 [==============================] - 0s 55us/sample - loss: 0.0157 - mae: 0.0907 - mse: 0.0157 - accuracy: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb55e88b278>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(normed_train_data, train_labels, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206/1206 [==============================] - 0s 10us/sample - loss: 0.5650 - mae: 0.5237 - mse: 0.5650 - accuracy: 0.6144\n",
      "test loss, test acc: [0.5650229968044097, 0.5237288, 0.565023, 0.61442786]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(normed_test_data, test_labels, batch_size=128)\n",
    "print('test loss, test acc:', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
